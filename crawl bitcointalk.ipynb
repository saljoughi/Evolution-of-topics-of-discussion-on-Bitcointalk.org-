{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit ('base': conda)",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "902f54bb636231c6c13f50ac8c26a592b81c0e939bbba3d8b69d514bd541ab5c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Process\n",
    "from random import shuffle\n",
    "import requests\n",
    "import ujson as json\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def crawl(url_list, procId):\n",
    "\n",
    "    posts = []\n",
    "    t = time.time()\n",
    "    for i, url in enumerate(url_list):\n",
    "        while True:\n",
    "            time.sleep(1.001) \n",
    "            res = requests.get(url)\n",
    "            if res.status_code == 200:\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(1.001)\n",
    "         \n",
    "        soup = BeautifulSoup(res.text, 'lxml')\n",
    "        \n",
    "        articles = soup.find('div', {'id': 'bodyarea'}).findAll('td', {'class':  ['windowbg', 'windowbg2']})\n",
    "        post_article = articles[0]\n",
    "        comment_articles = articles[1:]\n",
    "        \n",
    "        post = {}\n",
    "        try:\n",
    "            post['post_user'] = post_article.find('td', 'poster_info').find('b').text\n",
    "            post_article = post_article.find('td', 'td_headerandpost')\n",
    "            post_article_meta = post_article.find('table').findAll('div')\n",
    "            post['title'] = post_article_meta[0].text.strip()\n",
    "            post['posted_time'] = post_article_meta[1].text\n",
    "            post['post_body'] = post_article.find('div', 'post').text\n",
    "        except:\n",
    "            \n",
    "            continue\n",
    "\n",
    "        comment_list = []\n",
    "        for comment_article in comment_articles:\n",
    "            one_comment = {}\n",
    "            try:\n",
    "                one_comment['post_user'] = comment_article.find('td', 'poster_info').find('b').text\n",
    "            except:\n",
    "                print(url)\n",
    "                print(comment_article)\n",
    "\n",
    "            comment_article = comment_article.find('td', 'td_headerandpost')\n",
    "            post_body = comment_article.find('div', 'post').text\n",
    "            if post_body.isdigit():\n",
    "                # empty\n",
    "                continue\n",
    "            \n",
    "            one_comment['post_body'] = post_body\n",
    "            comment_article_meta = comment_article.find('table').findAll('div')\n",
    "            one_comment['title'] = comment_article_meta[0].text.strip()\n",
    "            one_comment['posted_time'] = comment_article_meta[1].text\n",
    "            comment_list.append(one_comment)\n",
    "\n",
    "        page_base_url = url.rpartition(\".\")[0]\n",
    "        current_comment_num = 20\n",
    "        prev_comment_page = '1'\n",
    "        while True:\n",
    "            #time.sleep(1.001)\n",
    "            comment_page_url = \"%s.%d\" % (page_base_url, current_comment_num)\n",
    "            while True:\n",
    "                time.sleep(1.001)\n",
    "                res_comment = requests.get(comment_page_url)\n",
    "                if res_comment.status_code == 200:\n",
    "                    break\n",
    "                else:\n",
    "                    time.sleep(1.001)\n",
    "                    \n",
    "            soup_comment = BeautifulSoup(res_comment.text, 'lxml')\n",
    "\n",
    "            current_page = soup_comment.find('div', {'id': 'bodyarea'}).find('table').find('b').text\n",
    "            if current_page == prev_comment_page:\n",
    "                break\n",
    "            else:\n",
    "                prev_comment_page = current_page\n",
    "                current_comment_num += 20\n",
    "\n",
    "            for comment_article in soup_comment.findAll('article'):\n",
    "                one_comment = {}\n",
    "                one_comment['post_user'] = comment_article.find('td', 'poster_info').find('b').text\n",
    "                comment_article = comment_article.find('td', 'td_headerandpost')\n",
    "                post_body = comment_article.find('div', 'post').text\n",
    "                if post_body.isdigit():\n",
    "                    \n",
    "                    continue\n",
    "\n",
    "                one_comment['post_body'] = post_body\n",
    "                comment_article_meta = comment_article.find('table').findAll('div')\n",
    "                one_comment['title'] = comment_article_meta[0].text.strip()\n",
    "                one_comment['posted_time'] = comment_article_meta[1].text\n",
    "                comment_list.append(one_comment)\n",
    "\n",
    "        post['comments'] = comment_list\n",
    "        posts.append(post)\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            t = time.time() - t\n",
    "            print(f\"{procId} - {i+1}/{len(url_list)}, {t:.2f} secondes\")\n",
    "            t = time.time()\n",
    "\n",
    "            if i > 0 and i % 1000 == 0:    \n",
    "                with open(f\"/home/davoud/bitcoin/bitcoin_forum_{procId}_{i//1000}.json\", 'w') as f:\n",
    "                    json.dump(posts, f)\n",
    "\n",
    "                posts = []\n",
    "                time.sleep(1.5)\n",
    "\n",
    "    time.sleep(1.5)\n",
    "    with open(f\"/home/davoud/bitcoin/bitcoin_forum_{procId}_last.json\", 'w') as f:\n",
    "                    json.dump(posts, f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists(\"./bitcoin\"):\n",
    "        os.mkdir(\"./bitcoin\")\n",
    "\n",
    "    page_url = \"https://bitcointalk.org/index.php?board=1.%d\"\n",
    "    url_list = []\n",
    "    last_page = 1337\n",
    "    print(\"crawl url lists...\", end=' ')\n",
    "    for page_num in range(0, last_page * 40, 40):            \n",
    "        while True:\n",
    "            time.sleep(1.001) \n",
    "            res = requests.get(page_url % page_num)            \n",
    "            if res.status_code == 200:\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(1.001)\n",
    "\n",
    "\n",
    "        page_soup = BeautifulSoup(res.text, 'lxml')\n",
    "        \n",
    "        if page_num == 0:\n",
    "            print(page_num)\n",
    "            table = page_soup.find('div', {'id': 'bodyarea'}).findAll('div', 'tborder')[1]\n",
    "        else:\n",
    "            print(page_num)\n",
    "            table = page_soup.find('div', {'id': 'bodyarea'}).findAll('div', 'tborder')[0]\n",
    "        for tr in table.findAll('tr')[1:]:\n",
    "            td = tr.findAll('td')[2]\n",
    "            if td.find('img') is not None:\n",
    "                # ignore announcement post\n",
    "                continue\n",
    "\n",
    "            url_list.append(td.find('a')['href'])\n",
    "\n",
    "    #shuffle(url_list)\n",
    "    print(len(url_list))\n",
    "    worker_num = 2\n",
    "    chunk_num = len(url_list) // worker_num\n",
    "    worker_list = []\n",
    "\n",
    "    for i in range(worker_num-1):\n",
    "        worker_list.append(Process(target=crawl, args=(url_list[i*chunk_num:(i+1)*chunk_num], i)))\n",
    "\n",
    "    worker_list.append(Process(target=crawl, args=(url_list[(worker_num-1)*chunk_num:], worker_num-1)))\n",
    "\n",
    "    for worker in worker_list:\n",
    "        worker.start()\n",
    "\n",
    "    for worker in worker_list:\n",
    "        worker.join()\n",
    "\n",
    "\n",
    "print ('done')\n",
    "\n"
   ]
  }
 ]
}