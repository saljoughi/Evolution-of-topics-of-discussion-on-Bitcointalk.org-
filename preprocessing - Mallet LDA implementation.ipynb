{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit ('base': conda)",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "902f54bb636231c6c13f50ac8c26a592b81c0e939bbba3d8b69d514bd541ab5c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the files\n",
    "\n",
    "timeframe='2019a'\n",
    "inputpath='/home/davoud/Documents/cs project/comment source/edited data oct/'\n",
    "input_here= inputpath+timeframe+'.csv'\n",
    "pathldavis = '/home/davoud/Documents/cs project/trial/October 22/' + timeframe + '.html'\n",
    "pathshortsumary1= '/home/davoud/Documents/cs project/trial/October 22/' +  timeframe  + '_all_comments'+ '.csv'\n",
    "pathshortsumary2= '/home/davoud/Documents/cs project/trial/October 22/' + timeframe  +'_summary_table'+  '.csv'\n",
    "pathshortsumary3= '/home/davoud/Documents/cs project/trial/October 22/' + timeframe  +'_sample_text'+  '.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries \n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import csv\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "#  stop-words list\n",
    "import stop_words\n",
    "stopwords = set(stop_words.get_stop_words('en'))\n",
    "stopwords=list(stopwords)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#add additional item to stopwords\n",
    "stopwords=list(stopwords)\n",
    "stop_words=stopwords\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use','bitcoin','bitcoins','would','com', 'quote', 'pmquote', 'amquote', 'just', 'don', 'one', 'thing', 'even', 'way', 'maybe', 'also', 'please', 'well', 'actually', 'something','going', 'anything', 'le', 'ever', 'say', 'see', 'likely', 'per', 'another', 'someone', 'let', 'anyone', 'doesn', 'include', 'doe', 'exactly', 'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december', 'like', 'said', 'guy', 'will', 'can', 'able', 'people', 'become', 'tell', 'hey', 'much', 'many', 'lol', 'lot', 'want', 'still', 'really', 'think', 'didn', 'isn', 'post', 'edited', 'could','http_www', 'use', 'get', 'ask', 'say', 'self', 'look',  'wonder', 'sound', 'make' ,'come' ,'do', 'go', 'probably' ,'need' ,'next' ,'day' , 'today' ,'tomorrow', 'yesterday' ,'month', 'week' ,'year' ,'total' ,'however' ,'similar' ,'half' ,'read' ,'forum' ,'room', 'enough', 'put', 'http' ,'www' , 'btc', 'call', 'cool',  'search', 'say' ,'talk' ,'name', 'somewhere', 'indeed', 'use','give', 'keep', 'user','member','go','often', 'unit','good', 'seem','completely', 'different', 'suggestion', 'complete', 'mention','small','big','find','sound','wish','Now', 'day','month','week','year','guy','one','sound','bad','old','sure','give','main','sense','open','close','use','tool','wrong','say','test','keep','lose','soon','pass','directly','send','receive','case','see','true','false','concern','must','should','could','obviously','stand','seemingly','present','high','low','excellent','seriously','now','know','take','Now', 'day','month','week','year','guy','one','sound','bad','old','sure','give','main','sense','open','close','use','tool','wrong','say','test','keep','lose','soon','pass','directly','send','receive','case','see','true','false','concern','must','should','could','obviously','stand','seemingly','present','high','low','excellent','seriously','now','know','take','try'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feeding  data source\n",
    "\n",
    "df = pd.read_csv(input_here)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Cleaning\n",
    "# Convert to list\n",
    "data = df.for_lda.values.tolist()\n",
    "\n",
    "print ('Initial Number of Comments = ',len(data) )\n",
    "\n",
    "# Remove Emails\n",
    "# Remove new line characters\n",
    "# Remove distracting single quotes\n",
    "k=0\n",
    "temp100=[]\n",
    "for sent in data:\n",
    "    try: \n",
    "        sent=str(sent)\n",
    "        sent= sent.lower()\n",
    "        sent = re.sub(r'http\\S+', ' ', sent) # Remove websites\n",
    "        sent=  re.sub(r'\\S+.(com|org)\\S+', '', sent) # Remove website2\n",
    "        \n",
    "        sent = re.sub('\\S*@\\S*\\s?', '', sent) # Remove Emails\n",
    "        sent = re.sub('\\s+', ' ', sent) # Remove new line characters\n",
    "        sent = re.sub(\"\\'\", \"\", sent) # Remove distracting single quotes\n",
    "        sent= re.sub( \"[^a-zA-Z]\", \" \", sent)\n",
    "        if len(sent.split())>5 :\n",
    "            temp100.append(sent)\n",
    "            k +=1\n",
    "\n",
    "    except TypeError:\n",
    "        continue \n",
    "data= temp100\n",
    "print ('Remaing Comment after pre Cleaning = ',k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ'])\n",
    "\n",
    "\n",
    "\n",
    "for item in data_lemmatized:\n",
    "    for element in item:\n",
    "        if len(element)<3:\n",
    "            item.remove(element)\n",
    "\n",
    "\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    " # Download File: http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "mallet_path = '/home/davoud/Documents/malletm/mallet-2.0.8/bin/mallet'  # update this path\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=15, id2word=id2word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word,\n",
    "                                               coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_lda_model= gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(mallet_lda_model, corpus, id2word)\n",
    "vis\n",
    "# save graph\n",
    "pyLDAvis.save_html(vis, pathldavis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show list of topics\n",
    "optimal_model=ldamallet\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))\n",
    "df_topic_per = pd.DataFrame(optimal_model.print_topics(num_words=10),columns=['Topic Number','Keywords Perc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)\n",
    "# save the list\n",
    "df_dominant_topic.to_csv(pathshortsumary1)\n",
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head(15)\n",
    "\n",
    "sent_topics_sorteddf_mallet.to_csv(pathshortsumary3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "#sent_topics_sorteddf_mallet\n",
    "\n",
    "\n",
    "df_dominant_123 = pd.concat([ topic_counts, topic_contribution,df_topic_per], axis=1)\n",
    "df_dominant_123.columns=['Count_of_Comments', 'Perc_Contrib','Topic_Num', 'Topic_Keywords']\n",
    "df_dominant_123.to_csv(pathshortsumary2)\n",
    "df_dominant_123"
   ]
  }
 ]
}